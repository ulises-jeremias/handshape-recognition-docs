Sign Language Recognition  seeks to devise systems capable of  translating videos of people speaking in sign language into text.

In recent years, new advances in machine learning using models such as convolutional and recurrent neural networks have improved our ability to tackle complex recognition problems such as speech recognition,  image classification  or object detection. However, sign language recognition cannot currently take full advantage  of state-of-the-art models, since  the availability of labeled, quality data for training models is very limited \cite{}.   

The process of recognizing a sign language consists of several steps, ranging from image preprocessing, body part detection, facial expression recognition, hand shape recognition,  language modeling and language translation. Of these steps, handshape recognition plays  the most crucial role in the interpretation of signs\cite{}.  Lack of data also impairs the development of accurate sign recognition models.

In this article we propose to evaluate and compare new methods devoted to deal with small and unlabeled data sets in order to improve the current state-of-the-art in hand shape recognition for sign language. 

Our approach consists of combining and comparing three different techniques for improving model performance in these conditions: data augmentation,  prototypical networks for few shot learning and semi supervised learning. Our data augmentation scheme consists of basic augmentation operators such as rotations, translations  and crops.  We compare this  with new technique approach.

    