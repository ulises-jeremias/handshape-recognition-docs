Sign Language Recognition is a field in the intersection of computer vision and language translation that seeks to create systems capable of translating videos of people speaking in sign language into text.

In recent years, new advances in machine learning using models such as convolutional and recurrent neural networks have improved our ability to tackle complex recognition problems such as speech recognition,  image classification  or object detection\cite{Goodfellow2016DL}. These advances are fueled by a combination   of improvements in three areas; better datasets, better models, and more compute power. While the last two are mostly independent of a particular field, the availability of quality datasets for a given field limits the application of these new advances.  For example, common image classification datasets such as MNIST, CIFAR10, CIFAR100 and ImageNet contain  thousands of examples per class \cite{cubuk2019autoaugment}.

The process of recognizing a sign language consists of several steps, ranging from image preprocessing, body part detection, facial expression recognition, handshape recognition,  language modeling and language translation. Of these steps, handshape recognition plays the most crucial role in the interpretation of signs\cite{koller16,ronchetti2016sign}. However, sign language recognition cannot currently take full advantage  of state-of-the-art models, since  the availability of labeled, quality data for training models is very limited \cite{koller16}. Lack of data also impairs the development of accurate handshape recognition models \cite{koller16}. 

In particular, Convolutional Neural Networks (CNN), a type of neural network that takes advantage  of convolutional layers to learn arbitrary convolutional filters, have proven very effective at image classification \cite{Goodfellow2016DL}, including the classification of handshapes in images \cite{quiroga2017study}.  However, in most  applications, convolutional neural networks are trained using  thousands of images per class.  In handshape recognition tasks, the datasets are considerably smaller and of lower quality, and therefore the performance of the models suffers accordingly \cite{koller16,tang2015real,quiroga2017study}. 

In this work we propose to evaluate and compare new methods devoted to deal with small datasets in order to improve the current state-of-the-art in hand shape recognition for sign language.

Our approach consists of combining and comparing two different techniques for improving model performance in these conditions: data augmentation   and prototypical networks for few shot learning and semi supervised learning. Our data augmentation scheme consists of basic augmentation operators such as rotations, translations  and crops.  We compare this  with new technique approach.

In the following subsection we summarize previous efforts on training CNN on handshape datasets. Section \ref{sec:datasetsmodels} describes the datasets and models  we employed in our experiments,  which are detailed along with results in Section \ref{sec:experiments},  and Section \ref{sec:conclusion}  contains the conclusion of our work.

\subsection{Related Work}

Recent  years have seen the rise in the use of deep learning models for sign language recognition, specifically the use of convolutional neural networks to extract image features or directly classified hand images. \cite{koller16}  trained a  CNN to recognize handshapes  from the  RWTH handshape dataset,  which contains 3200 labeled samples and 50 different classes.  The model was based on a pre-trained network with a VGG architecture, and employed a semi-supervised scheme to take advantage of approximately  one million weakly labeled images, achieving an accuracy of 85.50\%. This constitutes the first attempt at adapting a model to overcome the low availability of labeled images for training. \cite{Ronchetti2016}  employed a radon transform as a feature for an ad hoc classifier that employed clustering as a quantization step and K nearest  neighbors  for the final classification. They tested the model on the LSA16 dataset,  which contains only 800 examples, obtaining an accuracy of 92.3\%. \cite{quiroga2017study}  evaluated several CNNs on the LSA16 and RWTH datasets, including both  vanilla and pre-trained models. The use of pretrained models helps to alleviate the lack of labeled data, since pretraining the convolutional filters establishes a prior that a further classifier can exploit for handshape recognition. This work is the second and last instance we found where a specific strategy was employed to alleviate the lack of data. Their best models of an accuracy of 95.92\% for LSA16 and 82.88\%  for RWTH. \cite{ciarp2018} trained a simple neural network to classify a new dataset they created, which contains 6000 examples and 10 classes, reaching an accuracy of 99.20\%. \cite{tang2015real} train a CNN on a custom dataset with 36 classes, 8 subjects and 57000 sample images. However, the samples correspond to video sequences and therefore are highly correlated;  while there are approximately 2000 images per class, there are only eight image sequences, one for each subject. Each of this image sequences  contains approximately 250 images  which are highly correlated,  and therefore it is best to consider the dataset as having eight image sequences per class. They obtained an accuracy of 94.17\%, \cite{ameen2017fingerspelling} trained a simple CNN with only 6 layers using the ASL Finger spelling dataset, obtaining an accuracy of 80.34\%. The dataset consists of 60000 images of 25 different classes, but they were captured as videos so they are also highly correlated as in the previous case. \cite{barros14multichannel} employed the Jochen Triesch Database (JTD), which contains only 10 classes and  72 samples per class, as well as the NAO Camera Hand Posture Database, which contains 4 classes and 400 examples per class. They trained a simple CNN with a multichannel image containing the results of the Sobel operator as input, obtaining an F-score of 94\%  and  98\% in  each dataset perspective. \cite{alani2018peru}  trained a deep CNN on the Hand Gesture Dataset LPD,  which contains 3250 images of only 6 classes, obtaining an accuracy of 99.73\%. 

This brief review confirms our previous statement that  while CNN are being consistently applied to handshape recognition tasks, most of these datasets are small and ad hoc, that is, recorded specifically for the purpose of testing a single model and not developed with the intent of providing a benchmark and complete training set for handshape recognition models. It is also worth noticing that some datasets are so small that it is very easy to obtain near-perfect performance with simple models. Also, many datasets are not readily available,  given that the authors have not publish the data and do not provide any means of obtaining it.  We note that the RWTH and LSA16 are both publicly available and current models have been shown to achieve less than perfect accuracy for them.  While the dataset in \cite{ciarp2018}  has been easily solved,  it is interesting because it targets general handshapes instead of those specific to sign language. We will call this dataset CIARP.



    