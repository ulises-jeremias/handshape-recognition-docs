
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for then instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{float}
\usepackage{caption}
\usepackage{graphicx}
\graphicspath{{tex/images/}}

\usepackage[]{booktabs}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Recognizing Handshapes using Small and Unlabeled Datasets}

% a short form should be given in case it is too long for the running head
\titlerunning{Recognizing Handshapes using Small and Unlabeled Datasets}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Ulises Jeremias Cornejo Fandos \inst{1}\thanks{equal contribution}%
\and Gaston Gustavo Rios \inst{1, 3}\printfnsymbol{1} \and \\ Franco Ronchetti \inst{1} \and Facundo Quiroga \inst{1} \and Waldo Hasperué \inst{1,2}}
%
\authorrunning{Cornejo \and Rios \and Ronchetti \and Quiroga \and Hasperué al.}
% (feature abused for this document to repeat the title also on left hand pages)

\def\aa{\}}
\def\ab{\{}

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{
$^{1}$ Facultad de Informática, Universidad Nacional de La Plata \\
$^{2}$  Investigador Asociado - Comisión de Investigaciones Científicas (CIC) \\
$^{3}$  Becario de entrenamiento - Comisión de Investigaciones Científicas (CIC)
 \\ \email{\ab fquiroga,fronchetti,whasperue\aa @lidi.info.unlp.edu.ar}
}

%holooo
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\maketitle


\input{tex/abstract.tex}

\section{Introduction}
\input{tex/intro.tex}

\section{Datasets and Models}
\subsection{Datasets}
\input{tex/datasets.tex}

\subsection{Models}
\input{tex/background.tex}

\section{Experiments}
For our experiments we will use the LSA and RWTH datasets. For data preprocessing we divide each image by 255 to get them to fit a range of [0,1], then we apply normalization feature-wise substracting the mean  and dividing by the standard deviation of each feature. We made experiments with data augmentation and without it. The data augmentation used includes flipping the images, changing their shape by a factor of 0.2 and rotating a maximum of 30 degrees.
\subsection{Setup}

\subsection{Results}

\begin{table}[h!]
\centering
\begin{tabular}{ p{18em} p{7em} p{8em} p{8em}}
\toprule
\emph{Method} & \emph{LSA16} &  \emph{RWTH}  &  \emph{CIARP} \\ \midrule
CIARP pAPER (https://link.springer.com/chapter/10.1007/978-3-319-75193-1\_53)* \cite{CIARP_resultados} & - & - &  \\
DeepHand* \cite{koller2016deep} & - & 85.50 \\
VGG16* \cite{Quiroga_blablabla} & \textbf{95.92} & \textbf{82.88} \\
DenseNet \cite{} & NN.NN & NN.NN \\
Prototypical Networks \cite{} & NN.NN & NN.NN \\
DenseNet ++ \cite{} & NN.NN & NN.NN \\
Prototypical Networks ++\cite{} & NN.NN & NN.NN \\
DenseNet + Unlabeled \cite{} & NN.NN & NN.NN \\
Prototypical Networks + Unlabeled \cite{} & NN.NN & NN.NN \\
\bottomrule
\end{tabular}
\caption{Accuracy of various convolutional neural network models on two datasets: LSA16 \cite{ronchetti2016a} and handshapes from RWTH-PHOENIX-Weather \cite{koller2016deep}. Results from methods annotated with * were taken from other papers. Models with "++" used data augmentation as described in section XXXX. \label{tab:results}}
\end{table}



\section{Conclusion}

\begin{thebibliography}{4}
    \bibitem{matchingnet} \href{https://arxiv.org/abs/1606.04080}{Matching Networks for One Shot Learning}. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra.
    \bibitem{protonet} \href{https://arxiv.org/abs/1703.05175}{Prototypical Networks for Few-shot Learning}. Jake Snell, Kevin Swersky, Richard S. Zemel.
    \bibitem{omniglot} \href{https://github.com/brendenlake/omniglot}{Omniglot dataset}.
    \bibitem{densenet} \href{https://arxiv.org/pdf/1608.06993.pdf}{Densely Connected Convolutional Networks}. Gao Huang et al.
    \bibitem{densenet_cifar} \href{https://arxiv.org/pdf/1802.03268.pdf}{Efficient Neural Architecture Search via Parameter Sharing}. Hieu Pham et al.
    \bibitem{senet} \href{https://arxiv.org/pdf/1709.01507.pdf}{Squeeze-and-Excitation Networks}
    \bibitem{data_augmentation} \href{https://books.google.com.ar/books?id=rERADwAAQBAJ&pg=PA64&lpg=PA64&dq=Variance+is+the+algorithm\%E2\%80\%99s+tendency+to+learn+random+things+irrespective+of+the+real+signal+by+fitting+highly+flexible+models+that+follow+the+error&source=bl&ots=raekkRHUoA&sig=ACfU3U0m3wuVS6hW9WDDsq4SGP8rSDK6Fg&hl=es-419&sa=X&ved=2ahUKEwjw8Z73q4rjAhVvD7kGHa4jDnwQ6AEwBXoECAkQAQ#v=onepage&q&f=false}{Machine Learning with R}. Abhijit Ghatak.
    \bibitem{wide_resnet}
    \href{https://arxiv.org/pdf/1605.07146.pdf}{Wide Residual Networks}. Sergey Zagoruyko, Nikos Komodakis.
\end{thebibliography}

\end{document}
